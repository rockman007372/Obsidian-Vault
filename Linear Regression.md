---
tags: processed
course: CS2109S
type: lecture
date: 2022-09-19 Monday
---
Links: [[CS2109S]]
- - -

## Definition
- Linear regression is a class of [[Supervised Learning]]
- Given inputs over a continuous range, predict result within a continuous output. 
- Example: Given the area of a property, predict its price.
- Goal of regression specifically and supervised ML in general:

> Given a **training set** of N example input-output pairs $(x_1, y_2), (x_2, y_2), \text{...}$ where each $y_j$ is generated by an unknown $x_j$ through a function $y = f(x)$, **discover a function h that approximates** the true function f.

## Univariate Linear Regression

A univariate linear function with input $x$ and output  $y$ has the form:
$$y = w_1x + w_0$$

Where `w0` and `w1` are real-valued coefficients to be learned. We use the letter `w` because we can think of the coefficients as **weights**; the value of `y` is changed by changing the relative weight of one term to another. We can define **`w`** to be the vector `[w0, w1]` and define the hypothesis function:
$$h_w(x) = w_1x + w_0$$

The task of finding the $h_w$ that best fits the data is called **linear regression**. To fit a line to the data, all we have to do is find the values of the weights that **minimize the empirical loss**.

Since we can draw many best fit lines over the data, we follow **Occam's Razor Theorem** → prefer the **simplest** hypothesis that is consistent with the data.

One empirical cost function we can use is the **squared cost function**:
$$J(w_{0},w_{1})= \frac{1}{2m}\sum\limits^m_{i=1}(h_{w}(x^{(i)})-y^{(i)})^2$$
Where:
- m is the training set number $(x_1, y_2), (x_2, y_2), \text{...}$
- $x^{(i)}$ is the input i and $y^{(i)}$ is the corresponding output

Our goal is to pick $w_{0}, w_{1}$ such that $J(w_{0},w_{1})$ is minimised!

We can define graph loss as a function of $w_0$ and $w_1$ is a three dimensional space, and our goal is to find the **global minimum**!

![[Pasted image 20220919232206.png]]

## Gradient Descent

Gradient descent is a technique to find the global minimum in the **weight space** - the space defined by all possible settings of the weights. It is very similar to [[Hill Climbing]] from Local Search - We first start at some point $(w_0, w_1)$, pick a nearby point that reduces $J(w_0, w_1)$ and keep going until hopefully we arrive at global minimum.

![[Pasted image 20220919232629.png]]

Learning rule for the weights:

$$w_{j}← w_{j}- \alpha\frac{\partial{J(w_{0}, w_{1},...))}}{\partial{w_{j}}}$$
where:
- $\alpha$ is the learning rate, used to minimize the loss (determines how fast we descend down to the minimum) → Can be fixed or vary with gradient
- $\partial$ is the partial derivative - differentiation with respect to a **single variable**

Correct way to perform gradient descent:

![[Pasted image 20220919233315.png]]

![[Pasted image 20220919235121.png]]

##### Intuition of why this works

![[Pasted image 20220919233458.png]]

##### Choosing a good learning rate is important
If apha is too small, it will take a long time to reach minimum
![[Pasted image 20220919233912.png]]

If alpha is too large, values will fluctuate and overshoot.
![[Pasted image 20220919234002.png]]

![[Regression 2022-09-19 23.40.30.excalidraw||400x900]]

Ideally, alpha should **decrease** as gradient decreases:
- Initially, large alpha to speed up descent
- Nearing optimal value, slow down for more gradual convergence

##### Variants of Gradient Descent
- **Batch** Gradient Descent: Consider **all** training sets (ABOVE)
- **Stoichastic**: 
	- Only consider one data point at a time
	- Faster, but more random and can escape local minima

## Generalized Linear Regression

Instead of modeling hypothesis as a univariate function, we can have hypothesis functions with multiple variables:

![[Pasted image 20220919235703.png]]

where:
- n is the number of feature
- $x^{(i)}$ is the input features (**a (n x 1) matrix**) of the $i^{th}$ training set
- $x_j^{(i)}$ is the value of feature j in the  $i^{th}$ training set

We can express hypothesis function as a **vector**:

![[Pasted image 20220920000358.png]]

We perform gradient descent in the generalised case similar to the univariate case:

![[Pasted image 20220920000502.png]]

### Feature Scaling using Mean Normalization

When features have significantly different scales, Gradient Descent does not work very well:
- Features with larger scales will affect the loss more significantly and influences the hypothesis curve more.
- Hence we scale the features so that they have roughly the same scale.
- We try to aim for: $-1 \leq x_i \leq 1$

![[Pasted image 20220920001038.png]]

We can also use mean normalizatiton:
![[Pasted image 20220920001109.png]]

## Polynomial Regression

Besides linear regression, hypothesis can also be modelled as a polynomial regression:

![[Pasted image 20220920001607.png]]

In real-world data, a straight line might not fit the data perfectly. Consider the relation between **number of schools nearby** and **asking price of a house**.

Houses with 0 schools nearby tend to be cheaper than houses with 1 school nearby. However, **as the number of schools increases, the increase in prices decrease**. A polynomial function can better capture this relationship. A polynomial function is written as follows:
$$ y = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n $$

where $y$ is the target value, $x$ is a (*single*) feature value, and $n$ is the degree of the polynomial. $w_0$ and $w_1, \dots, w_n$ are as before.

- We can find the weights using either gradient descent or normalization.
- The polynomial does not have to be discrete → Instead of $x^3$, we can perhaps have $\sqrt{x}$
- As we create a higher degree polynomial matrix, each column will have a larger scale than the previous one. This can lead to poor performance for gradient descent → **feature scaling** 

## Normalization

Normalization is **another method** besides Gradient Descent to find the **weights** of hypothesis in order to find the global minimum / minimize loss!

Suppose we have  m examples: $$(x^{(1)}, y^{(1)}), \cdot \cdot \cdot , (x^{(m)}, y^{(m)}))$$
and n features:
$$x^{(i)} = 
\begin{pmatrix}
x^{(i)}_{0} \\
x^{(i)}_{1} \\
\vdots \\
x^{(i)}_{n}
\end{pmatrix}$$
Apply normalization equation:

![[Regression 2022-09-20 00.26.21.excalidraw]]

## Summary

## Questions/Cues

- Coursera [Quiz](https://github.com/mGalarnyk/datasciencecoursera/blob/master/Stanford_Machine_Learning/Week2/week2quiz1LinearRegressionMultipleVariables.md)
